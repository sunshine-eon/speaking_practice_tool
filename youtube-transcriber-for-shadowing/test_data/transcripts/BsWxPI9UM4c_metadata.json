{
  "video_id": "BsWxPI9UM4c",
  "title": "Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar",
  "url": "https://www.youtube.com/watch?v=BsWxPI9UM4c",
  "duration": 6393,
  "chapters": [
    {
      "chapter_index": 1,
      "title": "Introduction to Hamel and Shreya",
      "start_time": 0.0,
      "end_time": 297.0,
      "start_time_formatted": "00:00",
      "end_time_formatted": "04:57"
    },
    {
      "chapter_index": 2,
      "title": "What are evals?",
      "start_time": 297.0,
      "end_time": 596.0,
      "start_time_formatted": "04:57",
      "end_time_formatted": "09:56"
    },
    {
      "chapter_index": 3,
      "title": "Demo: Examining real traces from a property management AI assistant",
      "start_time": 596.0,
      "end_time": 1011.0,
      "start_time_formatted": "09:56",
      "end_time_formatted": "16:51"
    },
    {
      "chapter_index": 4,
      "title": "Writing notes on errors",
      "start_time": 1011.0,
      "end_time": 1434.0,
      "start_time_formatted": "16:51",
      "end_time_formatted": "23:54"
    },
    {
      "chapter_index": 5,
      "title": "Why LLMs can’t replace humans in the initial error analysis",
      "start_time": 1434.0,
      "end_time": 1516.0,
      "start_time_formatted": "23:54",
      "end_time_formatted": "25:16"
    },
    {
      "chapter_index": 6,
      "title": "The concept of a “benevolent dictator” in the eval process",
      "start_time": 1516.0,
      "end_time": 1687.0,
      "start_time_formatted": "25:16",
      "end_time_formatted": "28:07"
    },
    {
      "chapter_index": 7,
      "title": "Theoretical saturation: when to stop",
      "start_time": 1687.0,
      "end_time": 1899.0,
      "start_time_formatted": "28:07",
      "end_time_formatted": "31:39"
    },
    {
      "chapter_index": 8,
      "title": "Using axial codes to help categorize and synthesize error notes",
      "start_time": 1899.0,
      "end_time": 2679.0,
      "start_time_formatted": "31:39",
      "end_time_formatted": "44:39"
    },
    {
      "chapter_index": 9,
      "title": "The results",
      "start_time": 2679.0,
      "end_time": 2766.0,
      "start_time_formatted": "44:39",
      "end_time_formatted": "46:06"
    },
    {
      "chapter_index": 10,
      "title": "Building an LLM-as-judge to evaluate specific failure modes",
      "start_time": 2766.0,
      "end_time": 2911.0,
      "start_time_formatted": "46:06",
      "end_time_formatted": "48:31"
    },
    {
      "chapter_index": 11,
      "title": "The difference between code-based evals and LLM-as-judge",
      "start_time": 2911.0,
      "end_time": 3130.0,
      "start_time_formatted": "48:31",
      "end_time_formatted": "52:10"
    },
    {
      "chapter_index": 12,
      "title": "Example: LLM-as-judge",
      "start_time": 3130.0,
      "end_time": 3285.0,
      "start_time_formatted": "52:10",
      "end_time_formatted": "54:45"
    },
    {
      "chapter_index": 13,
      "title": "Testing your LLM judge against human judgment",
      "start_time": 3285.0,
      "end_time": 3651.0,
      "start_time_formatted": "54:45",
      "end_time_formatted": "60:51"
    },
    {
      "chapter_index": 14,
      "title": "Why evals are the new PRDs for AI products",
      "start_time": 3651.0,
      "end_time": 3909.0,
      "start_time_formatted": "60:51",
      "end_time_formatted": "65:09"
    },
    {
      "chapter_index": 15,
      "title": "How many evals you actually need",
      "start_time": 3909.0,
      "end_time": 4061.0,
      "start_time_formatted": "65:09",
      "end_time_formatted": "67:41"
    },
    {
      "chapter_index": 16,
      "title": "What comes after evals",
      "start_time": 4061.0,
      "end_time": 4197.0,
      "start_time_formatted": "67:41",
      "end_time_formatted": "69:57"
    },
    {
      "chapter_index": 17,
      "title": "The great evals debate",
      "start_time": 4197.0,
      "end_time": 4515.0,
      "start_time_formatted": "69:57",
      "end_time_formatted": "75:15"
    },
    {
      "chapter_index": 18,
      "title": "Why dogfooding isn’t enough for most AI products",
      "start_time": 4515.0,
      "end_time": 4703.0,
      "start_time_formatted": "75:15",
      "end_time_formatted": "78:23"
    },
    {
      "chapter_index": 19,
      "title": "OpenAI’s Statsig acquisition",
      "start_time": 4703.0,
      "end_time": 4982.0,
      "start_time_formatted": "78:23",
      "end_time_formatted": "83:02"
    },
    {
      "chapter_index": 20,
      "title": "The Claude Code controversy and the importance of context",
      "start_time": 4982.0,
      "end_time": 5053.0,
      "start_time_formatted": "83:02",
      "end_time_formatted": "84:13"
    },
    {
      "chapter_index": 21,
      "title": "Common misconceptions around evals",
      "start_time": 5053.0,
      "end_time": 5437.0,
      "start_time_formatted": "84:13",
      "end_time_formatted": "90:37"
    },
    {
      "chapter_index": 22,
      "title": "The time investment",
      "start_time": 5437.0,
      "end_time": 5618.0,
      "start_time_formatted": "90:37",
      "end_time_formatted": "93:38"
    },
    {
      "chapter_index": 23,
      "title": "Overview of their comprehensive evals course",
      "start_time": 5618.0,
      "end_time": 5877.0,
      "start_time_formatted": "93:38",
      "end_time_formatted": "97:57"
    },
    {
      "chapter_index": 24,
      "title": "Lightning round and final thoughts",
      "start_time": 5877.0,
      "end_time": 6393,
      "start_time_formatted": "97:57",
      "end_time_formatted": "106:33"
    }
  ]
}