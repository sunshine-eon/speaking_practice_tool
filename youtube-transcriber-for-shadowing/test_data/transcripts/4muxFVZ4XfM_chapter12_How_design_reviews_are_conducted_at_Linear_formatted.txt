Chapter 12: How design reviews are conducted at Linear
Video: Inside Linear: Building with taste, craft, and focus | Karri Saarinen (co-founder, designer, CEO)
Time: 33:08 - 38:34
============================================================

Coming back to design briefly, just like very practically, how do you guys do design reviews? How do you actually go about reviewing what's going on? This may be too big a question, but whatever you can share would be helpful. How do you know when it's done? How do you know when it's ready and approved? 

We've been exploring different ways of doing this. I still run the design team, so I see some of the designs on a weekly basis. One of the other co-founders or the head of product and I are basically the sponsors for the projects. We are responsible for reviewing the work. We might have a meeting where we go through the demo, and people can explain what's going on, how they think about it, and why. After that, we might provide feedback, noting anything that seems strange or off. 

Sometimes, after the initial stages, we won't start fixing everything right away. It's more about getting the main concept there and figuring out how it works. Before we launch, I might go in and try it out, clicking around and testing different states. Occasionally, I find issues. For example, we were building a threading feature for comments, and while it looked good in the demos, I noticed that when I tried it with different message lengths, the animations were janky, or the screen didn't scroll correctly. I captured those issues and sent them to the team, which led us to pull back the release until those problems were fixed. 

That project was a simple concept, something well-known like threading comments, so it was mostly about execution. However, we also have projects where we're not sure how things should work. For instance, we built a feature for project updates, which is common in many companies. They need to write updates on a project, indicating whether it's yellow, green, or red. Companies have different ways of doing this in various tools. We thought it would be really cool if, inside Linear, the team could write updates and capture some stats about what actually happened. 

That feature has been working well, but after using it for a while, we realized there could be a more robust way of following these updates. Maybe leadership could receive updates via email, or we could implement a search or filtering system for when there are a lot of updates. Often, we think about the initial scope and are okay with launching it, knowing it's not the fully figured-out version. We need to see how people use it and gather feedback.

So, it sounds like the decision of whether it goes out or not is based on an intuitive feeling from your actual experience trying it out. Is that correct? 

Yes, I would say a lot of what we do is based on that intuition. We don't do A/B testing or specifically follow certain metrics. Sometimes we have telemetry to see how people use certain features, but that's not usually our primary goal. It's more about understanding the problem we have and determining if this solution feels right and is good enough to be released to customers.

One more question along this thread: how do you actually structure these reviews? It sounds like you go straight to a prototype. Is there a design review phase, or is it all kind of informal where people just review and provide feedback? 

Projects don't necessarily have specific states, but we usually start with designs. There are explorations on the design, and we might have different approaches, or sometimes it's clear with just one way. As I mentioned before, we try to get into the building phase as quickly as possible. This allows us to see if the direction is reasonable and what problems it might cause. 

There aren't specific review stages; it's more about checking in on the project every week or every two weeks. Before releasing, we also conduct a review to really test it out and ensure it meets the quality we want.